{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b29cb9f2-e3c0-44cc-8327-7757c5add287",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Install all needed dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bff9c793-7ca5-4f3b-8353-b55d3acb3b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "codeflare-sdk 0.26.0 requires pydantic<2, but you have pydantic 2.11.1 which is incompatible.\n",
      "kfp 2.9.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet --upgrade langchain_community langchain-huggingface vllm==v0.6.4.post1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ac090bc-a3fa-4bd3-a385-ff597777a8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install the YAML magic\n",
    "!pip install --quiet yamlmagic\n",
    "%load_ext yamlmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1918ea4-87c8-4880-900a-134a0bbb5591",
   "metadata": {},
   "source": [
    "Configuration and used models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f0abf06-145f-4644-b25d-823c6ffc58af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            require(\n",
       "                [\n",
       "                    \"notebook/js/codecell\",\n",
       "                    \"codemirror/mode/yaml/yaml\"\n",
       "                ],\n",
       "                function(cc){\n",
       "                    cc.CodeCell.options_default.highlight_modes.magic_yaml = {\n",
       "                        reg: [\"^%%yaml\"]\n",
       "                    }\n",
       "                }\n",
       "            );\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%yaml parameters\n",
    "\n",
    "# Model\n",
    "embedder_model_name_or_path: ibm-granite/granite-embedding-30m-english\n",
    "generator_model_name_or_path: ibm-granite/granite-3.2-2b-instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ef69a7-c616-4b06-b1ad-d3cb98abe7df",
   "metadata": {},
   "source": [
    "#  Langchain RAG\n",
    "\n",
    "Prepare list of documents containing chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb54a2f0-aef6-4308-a8b4-07e9c7cca23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 150 entries\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "link = \"https://huggingface.co/ngxson/demo_simple_rag_py/raw/main/cat-facts.txt\"\n",
    "documents = []\n",
    "\n",
    "# Retrieve knowledge from provided link, use every line as a separate chunk.\n",
    "for line in urllib.request.urlopen(link):\n",
    "    documents.append(\n",
    "        Document(\n",
    "            page_content=line.decode('utf-8'),\n",
    "            metadata={\"source\": \"cats\", \"doc_id\": \"cats\"}\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(f'Loaded {len(documents)} entries')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7676b550-1247-4c90-aa0d-44ca9561a6d9",
   "metadata": {},
   "source": [
    "Initialize vector store and fill it with chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "286c9e72-c921-489a-b977-8df7d964ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=parameters['embedder_model_name_or_path'],\n",
    ")\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "document_ids = vector_store.add_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5bb3cd-9785-43fa-b1c4-e16e78b69073",
   "metadata": {},
   "source": [
    "**Specify user query here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dc40487-bf1c-49ed-9106-0dc46e38820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_query = \"tell me about cat mummies\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677c95fe-1d36-4dfe-bf0d-1283857e5ee7",
   "metadata": {},
   "source": [
    "Using similarity search to retrieve most related chunks from vector database.\n",
    "\n",
    "This part is used to show what context is retrieved by vector store retriever. The actual retrieving is done as part of langchain pipeline below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c118e29-7fbf-4741-a474-3e5a3d46d8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved knowledge:\n",
      "page_content='In ancient Egypt, mummies were made of cats, and embalmed mice were placed with them in their tombs. In one ancient city, over 300,000 cat mummies were found.\n",
      "' metadata={'source': 'cats', 'doc_id': 'cats'}\n",
      "page_content='In 1888, more than 300,000 mummified cats were found an Egyptian cemetery. They were stripped of their wrappings and carted off to be used by farmers in England and the U.S. for fertilizer.\n",
      "' metadata={'source': 'cats', 'doc_id': 'cats'}\n",
      "page_content='When a family cat died in ancient Egypt, family members would mourn by shaving off their eyebrows. They also held elaborate funerals during which they drank wine and beat their breasts. The cat was embalmed with a sculpted wooden mask and the tiny mummy was placed in the family tomb or in a pet cemetery with tiny mummies of mice.\n",
      "' metadata={'source': 'cats', 'doc_id': 'cats'}\n",
      "page_content='Mohammed loved cats and reportedly his favorite cat, Muezza, was a tabby. Legend says that tabby cats have an “M” for Mohammed on top of their heads because Mohammad would often rest his hand on the cat’s head.\n",
      "' metadata={'source': 'cats', 'doc_id': 'cats'}\n"
     ]
    }
   ],
   "source": [
    "docs = vector_store.similarity_search(input_query)\n",
    "\n",
    "print('Retrieved knowledge:')\n",
    "for doc in docs:\n",
    "  print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52d2349-02aa-47d6-a5d0-783e8361feee",
   "metadata": {},
   "source": [
    "Initialize local vLLM and construct RAG chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c61b305-5f62-4f99-8577-0708ba5e5f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-01 12:49:21 arg_utils.py:1013] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 04-01 12:49:21 config.py:1136] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 04-01 12:49:21 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='ibm-granite/granite-3.2-2b-instruct', speculative_config=None, tokenizer='ibm-granite/granite-3.2-2b-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=ibm-granite/granite-3.2-2b-instruct, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n",
      "INFO 04-01 12:49:21 selector.py:135] Using Flash Attention backend.\n",
      "INFO 04-01 12:49:22 model_runner.py:1072] Starting to load model ibm-granite/granite-3.2-2b-instruct...\n",
      "INFO 04-01 12:49:22 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf66b9136b248e998bdf0726f05559e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-01 12:49:24 model_runner.py:1077] Loading model weights took 4.7349 GB\n",
      "INFO 04-01 12:49:24 worker.py:232] Memory profiling results: total_gpu_memory=79.14GiB initial_memory_usage=5.38GiB peak_torch_memory=5.30GiB memory_usage_post_profile=5.39GiB non_torch_memory=0.53GiB kv_cache_size=65.39GiB gpu_memory_utilization=0.90\n",
      "INFO 04-01 12:49:24 gpu_executor.py:113] # GPU blocks: 53568, # CPU blocks: 3276\n",
      "INFO 04-01 12:49:24 gpu_executor.py:117] Maximum concurrency for 131072 tokens per request: 6.54x\n",
      "INFO 04-01 12:49:29 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 04-01 12:49:29 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 04-01 12:49:45 model_runner.py:1518] Graph capturing finished in 15 secs, took 0.31 GiB\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import VLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "import transformers\n",
    "\n",
    "\n",
    "llm = VLLM(\n",
    "    model=parameters['generator_model_name_or_path'],\n",
    "    trust_remote_code=True,\n",
    "    max_new_tokens=1024,\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(parameters['generator_model_name_or_path'])\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Construct system prompt for inference providing retrieved chunks as context.\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    conversation=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"{input}\",\n",
    "    }],\n",
    "    documents=[{\n",
    "        \"title\": \"placeholder\",\n",
    "        \"text\": \"{context}\",\n",
    "    }],\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "prompt_template = PromptTemplate.from_template(template=prompt)\n",
    "\n",
    "# Wrap retrieved chunks using prompt template\n",
    "document_prompt_template = PromptTemplate.from_template(template=\"\"\"\\\n",
    "Document {doc_id}\n",
    "{page_content}\"\"\")\n",
    "document_separator=\"\\n\\n\"\n",
    "\n",
    "# Create retrieval chain\n",
    "combine_docs_chain = create_stuff_documents_chain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template,\n",
    "    document_prompt=document_prompt_template,\n",
    "    document_separator=document_separator,\n",
    ")\n",
    "rag_chain = create_retrieval_chain(\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    combine_docs_chain=combine_docs_chain,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "825212b5-0f8a-4743-9ab5-a06a76915587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.76s/it, est. speed input: 148.71 toks/s, output: 140.38 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat mummies have a significant historical and cultural importance, particularly in ancient Egypt. Here are some key points:\n",
      "\n",
      "1. **Mummification Practice**: In ancient Egypt, cats were mummified as part of their funeral rituals. This practice was not limited to cats alone; mice were also embalmed and placed in the tombs of cats.\n",
      "\n",
      "2. **Abundance**: Over 300,000 cat mummies were discovered in a single Egyptian cemetery in 1888. This suggests a widespread practice of cat mummification.\n",
      "\n",
      "3. **Use in Fertilizer**: After being stripped of their wrappings, these cat mummies were exported to England and the U.S. for use as fertilizer.\n",
      "\n",
      "4. **Mourning Rituals**: When a family cat died, Egyptians would mourn by shaving off their eyebrows and holding elaborate funerals. They would drink wine and beat their breasts during these ceremonies.\n",
      "\n",
      "5. **Placement**: The tiny mummified cat was often placed in the family tomb or in a pet cemetery, sometimes alongside mummies of mice.\n",
      "\n",
      "6. **Cultural Significance**: Tabby cats, with their distinctive coat pattern, are said to have an \"M\" for Mohammed on their heads due to a legend involving Prophet Mohammed's favorite cat, Muezza.\n",
      "\n",
      "These cat mummies not only provide insights into ancient Egyptian funeral practices but also reflect the cultural significance of cats in their society.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = rag_chain.invoke({\"input\": input_query})\n",
    "\n",
    "print(output['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721db740-b0a1-4889-8c62-551e0a7eb44f",
   "metadata": {},
   "source": [
    "# Cleaning Up\n",
    "\n",
    "Delete pipeline and associated model from GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d56878c-98e6-4018-b8a1-ac3fcfac08d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "del rag_chain, combine_docs_chain, llm\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
