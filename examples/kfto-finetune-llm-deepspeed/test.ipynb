{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a1222715-691b-4f9e-81cf-ac8a4e300831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func():\n",
    "    import os\n",
    "    import logging\n",
    "    from transformers import (\n",
    "        AutoModelForCausalLM,\n",
    "        AutoTokenizer,\n",
    "        TrainingArguments,\n",
    "        DataCollatorForLanguageModeling,\n",
    "        Trainer,\n",
    "    )\n",
    "    from datasets import load_dataset\n",
    "    from datasets.distributed import split_dataset_by_node\n",
    "    from peft import LoraConfig, get_peft_model\n",
    "\n",
    "    log_formatter = logging.Formatter(\n",
    "        \"%(asctime)s %(levelname)-8s %(message)s\", \"%Y-%m-%dT%H:%M:%SZ\"\n",
    "    )\n",
    "    logger = logging.getLogger(__file__)\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(log_formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path=model_name,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        pretrained_model_name_or_path=model_name,\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Freeze model parameters\n",
    "#    for param in model.parameters():\n",
    "#        param.requires_grad = False\n",
    "\n",
    "    # Inspired by https://medium.com/@alexandros_chariton/how-to-fine-tune-llama-3-2-instruct-on-your-own-data-a-detailed-guide-e5f522f397d7\n",
    "    def format_dataset(example):\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": example['question']},\n",
    "            {\"role\": \"assistant\", \"content\": example['answer']}\n",
    "        ]\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        return {\"prompt\": prompt}\n",
    "\n",
    "    def tokenize_dataset(example):\n",
    "        tokens = tokenizer(example['prompt'], padding=\"max_length\")\n",
    "        # Set padding token labels to -100 to ignore them in loss calculation\n",
    "        tokens['labels'] = [\n",
    "            -100 if token == tokenizer.pad_token_id else token for token in tokens['input_ids']\n",
    "        ]\n",
    "        return tokens\n",
    "\n",
    "    dataset = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "    train_data = dataset[\"train\"].map(format_dataset)\n",
    "    eval_data = dataset[\"test\"].map(format_dataset)\n",
    "    train_data = train_data.map(tokenize_dataset, remove_columns=['question', 'answer', 'prompt'])\n",
    "    eval_data = eval_data.map(tokenize_dataset, remove_columns=['question', 'answer', 'prompt'])\n",
    "\n",
    "#    lora_config = LoraConfig(r=4, lora_alpha=16, lora_dropout=0.1, bias=\"none\")\n",
    "#    model.enable_input_require_grads()\n",
    "#    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=eval_data,\n",
    "        args=TrainingArguments(output_dir=\"/tmp\",\n",
    "                               per_device_train_batch_size=1,\n",
    "                               per_device_eval_batch_size=1,\n",
    "                               num_train_epochs=8,\n",
    "                               logging_dir=\"/logs\",\n",
    "                               eval_strategy=\"epoch\",\n",
    "                               save_strategy=\"no\"),\n",
    "    )\n",
    "\n",
    "    trainer.data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer,\n",
    "        pad_to_multiple_of=8,\n",
    "        mlm=False,\n",
    "    )\n",
    "\n",
    "    # Train and save the model.\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    logger.info(\"parallel_mode: '{0}'\".format(trainer.args.parallel_mode))\n",
    "    logger.info(\"is_model_parallel: '{0}'\".format(trainer.is_model_parallel))\n",
    "    logger.info(\"model_wrapped: '{0}'\".format(trainer.model_wrapped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "11154c3c-7c23-42a7-a3d1-890e4f648e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.training import TrainingClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ba2fa8dc-9aaf-4c6b-a44f-500f40909f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes.client import (\n",
    "    V1EnvVar,\n",
    "    V1EnvVarSource,\n",
    "    V1SecretKeySelector\n",
    ")\n",
    "\n",
    "TrainingClient().create_job(\n",
    "    job_kind=\"PyTorchJob\",\n",
    "    name=\"pytorch-ddp\",\n",
    "    train_func=train_func,\n",
    "    num_workers=1,\n",
    "    num_procs_per_worker=\"auto\",\n",
    "    resources_per_worker={\"gpu\": 4},\n",
    "    base_image=\"quay.io/modh/training:py311-cuda121-torch241\",\n",
    "    env_vars=[\n",
    "        V1EnvVar(name=\"HF_TOKEN\", value_from=V1EnvVarSource(secret_key_ref=V1SecretKeySelector(key=\"HF_TOKEN\", name=\"hf-token\"))),\n",
    "        V1EnvVar(name=\"NCCL_DEBUG\", value=\"INFO\"),\n",
    "#        V1EnvVar(name=\"TOKENIZERS_PARALLELISM\", value=\"false\"),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d731d9d4-d8d4-47a9-b204-497c97b86ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
